{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, mutual_info_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#import pymrmr\n",
    "#from pymrmr import mRMR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mrmr import mrmr_classif\n",
    "from ReliefF import ReliefF\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def find_nearest_neighbors(instance, data, k):\n",
    "    distances = [euclidean_distance(instance, x) for x in data]\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    return sorted_indices[1:k+1]\n",
    "\n",
    "def reliefF(X, y, k=3, num_iterations=100):\n",
    "    num_samples, num_features = X.shape\n",
    "    feature_weights = np.zeros(num_features)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        instance_idx = np.random.randint(0, num_samples)\n",
    "        instance = X.iloc[instance_idx].values\n",
    "        same_class_indices = np.where(y == y[instance_idx])[0]\n",
    "        different_class_indices = np.where(y != y[instance_idx])[0]\n",
    "\n",
    "        # Find k nearest neighbors from the same class\n",
    "        same_class_neighbors = find_nearest_neighbors(instance, X.iloc[same_class_indices].values, k)\n",
    "        # Find k nearest neighbors from different classes\n",
    "        different_class_neighbors = find_nearest_neighbors(instance, X.iloc[different_class_indices].values, k)\n",
    "\n",
    "        # Update feature weights\n",
    "        for feature in range(num_features):\n",
    "            nearest_same = np.mean(X.iloc[same_class_indices].values[:, feature][same_class_neighbors])\n",
    "            nearest_different = np.mean(X.iloc[different_class_indices].values[:, feature][different_class_neighbors])\n",
    "            feature_weights[feature] += abs(instance[feature] - nearest_same) - abs(instance[feature] - nearest_different)\n",
    "\n",
    "    feature_weights /= num_iterations\n",
    "    return feature_weights\n",
    "\n",
    "# Load your dataset and split it into features (X) and target variable (y)\n",
    "data = pd.read_csv('../data/diabetes.csv')\n",
    "X = data.drop(columns=['Outcome'])\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Chi-square test\n",
    "# Create the SelectKBest object with chi2 scoring function\n",
    "k_best = SelectKBest(score_func=chi2, k=4)\n",
    "\n",
    "# Fit the feature selector to the data\n",
    "k_best.fit(X, y)\n",
    "\n",
    "# Get the selected features (indexes of the selected features)\n",
    "selected_features = k_best.get_support(indices=True)\n",
    "\n",
    "# Transform the original data to retain only the selected features\n",
    "X_selected = k_best.transform(X)\n",
    "print(\"Chi-square : \")\n",
    "print(\"Selected features (indexes):\", selected_features)\n",
    "\n",
    "#Mutual information\n",
    "# Create the SelectKBest object with mutual_info_classif scoring function\n",
    "k_best = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "\n",
    "# Fit the feature selector to the data\n",
    "k_best.fit(X, y)\n",
    "\n",
    "# Get the selected features (indexes of the selected features)\n",
    "selected_features = k_best.get_support(indices=True)\n",
    "\n",
    "# Transform the original data to retain only the selected features\n",
    "X_selected = k_best.transform(X)\n",
    "print(\"Mutual information : \")\n",
    "print(\"Selected features (indexes):\", selected_features)\n",
    "\n",
    "#mrmr\n",
    "selected_features = mrmr_classif(X=X, y=y, K=4)\n",
    "print(\"mrmr : \")\n",
    "print(selected_features)\n",
    "\n",
    "#reliefF\n",
    "if __name__ == \"__main__\":\n",
    "    feature_weights = reliefF(X, y, k=3, num_iterations=100)\n",
    "    print(\"ReliefF : \")\n",
    "    print(\"Feature weights:\", feature_weights)\n",
    "\n",
    "\n",
    "#sfs\n",
    "# Create a classifier (replace this with the classifier of your choice)\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Create the SequentialFeatureSelector object with forward selection\n",
    "sfs = SFS(clf,\n",
    "          k_features=(1, 8),  # Range of features to select (1 to all features)\n",
    "          forward=True,  # Forward selection (can also use backward=False for backward selection)\n",
    "          floating=False,  # Disable floating search\n",
    "          scoring='accuracy',  # Scoring metric for feature selection\n",
    "          cv=5)  # Cross-validation folds\n",
    "\n",
    "# Fit the SequentialFeatureSelector to the training data\n",
    "sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = sfs.k_feature_idx_\n",
    "\n",
    "# Transform the original data to retain only the selected features\n",
    "X_train_selected = sfs.transform(X_train)\n",
    "X_test_selected = sfs.transform(X_test)\n",
    "\n",
    "print(\"SFS : \")\n",
    "print(\"Selected feature indices:\", selected_feature_indices)\n",
    "#print(\"Selected feature names:\", data.feature_names[selected_feature_indices])\n",
    "\n",
    "#sbs\n",
    "model = LogisticRegression()  # You can use any other model of your choice\n",
    "\n",
    "# Initialize the step backward feature selector\n",
    "sbs = SFS(model,\n",
    "        k_features=(1, 8),\n",
    "        forward=False,  # Change to False for step backward selection\n",
    "        floating=False,\n",
    "        #verbose=2,\n",
    "        scoring='accuracy',  # Change this to your chosen metric\n",
    "        cv=5)  # Number of cross-validation folds\n",
    "\n",
    "# Perform step backward feature selection\n",
    "sbs.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = sbs.k_feature_idx_\n",
    "\n",
    "# Convert the indices to feature names\n",
    "selected_features = [X.columns[idx] for idx in selected_feature_indices]\n",
    "\n",
    "print(\"SBS : \")\n",
    "print(\"Selected features:\", selected_feature_indices)\n",
    "\n",
    "# Define a list of classification algorithms you want to run\n",
    "algorithms = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), svm.SVC(), RandomForestClassifier(), MLPClassifier(), GradientBoostingClassifier(), XGBClassifier(), LGBMClassifier(), CatBoostClassifier(verbose=False)]\n",
    "\n",
    "\n",
    "# Create a for loop to run each algorithm\n",
    "for algorithm in algorithms:\n",
    "    model_name = type(algorithm).__name__\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance using various metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"{model_name} evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Confusion Matrix: {confusion}\")\n",
    "    print(f\"Specificity: {specificity}\\n\")\n",
    "\n",
    "print(\"After select 4 features : \")\n",
    "\n",
    "X = data.drop(columns=['Pregnancies', 'BloodPressure', 'SkinThickness', 'DiabetesPedigreeFunction'])\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    model_name = type(algorithm).__name__\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = algorithm.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model's performance using various metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"{model_name} evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Confusion Matrix: {confusion}\")\n",
    "    print(f\"Specificity: {specificity}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
